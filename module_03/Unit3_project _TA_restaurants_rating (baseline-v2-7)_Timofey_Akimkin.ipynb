{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Загружаем функцию для генерирования полиномиальных признаков:\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.\nimport re #регулярные выражения\nfrom datetime import datetime as dat #работа с датами\n\nfrom random import randint\n\n\n\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-09-30T18:47:28.357249Z","iopub.execute_input":"2021-09-30T18:47:28.357561Z","iopub.status.idle":"2021-09-30T18:47:28.376939Z","shell.execute_reply.started":"2021-09-30T18:47:28.357504Z","shell.execute_reply":"2021-09-30T18:47:28.375499Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# Predict TripAdvisor Rating\n## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n**По ходу задачи:**\n* Прокачаем работу с pandas\n* Научимся работать с Kaggle Notebooks\n* Поймем как делать предобработку различных данных\n* Научимся работать с пропущенными данными (Nan)\n* Познакомимся с различными видами кодирования признаков\n* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n* И совсем немного затронем ML\n* И многое другое...   \n\n\n\n### И самое важное, все это вы сможете сделать самостоятельно!\n\n*Этот Ноутбук являетсся Примером/Шаблоном к этому соревнованию (Baseline) и не служит готовым решением!*   \nВы можете использовать его как основу для построения своего решения.\n\n> что такое baseline решение, зачем оно нужно и почему предоставлять baseline к соревнованию стало важным стандартом на kaggle и других площадках.   \n**baseline** создается больше как шаблон, где можно посмотреть как происходит обращение с входящими данными и что нужно получить на выходе. При этом МЛ начинка может быть достаточно простой, просто для примера. Это помогает быстрее приступить к самому МЛ, а не тратить ценное время на чисто инженерные задачи. \nТакже baseline являеться хорошей опорной точкой по метрике. Если твое решение хуже baseline - ты явно делаешь что-то не то и стоит попробовать другой путь) \n\nВ контексте нашего соревнования baseline идет с небольшими примерами того, что можно делать с данными, и с инструкцией, что делать дальше, чтобы улучшить результат.  Вообще готовым решением это сложно назвать, так как используются всего 2 самых простых признака (а остальные исключаются).","metadata":{}},{"cell_type":"markdown","source":"# import","metadata":{}},{"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 36","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:28.379034Z","iopub.execute_input":"2021-09-30T18:47:28.379310Z","iopub.status.idle":"2021-09-30T18:47:28.392543Z","shell.execute_reply.started":"2021-09-30T18:47:28.379257Z","shell.execute_reply":"2021-09-30T18:47:28.390931Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:28.393479Z","iopub.execute_input":"2021-09-30T18:47:28.393673Z","iopub.status.idle":"2021-09-30T18:47:29.945657Z","shell.execute_reply.started":"2021-09-30T18:47:28.393636Z","shell.execute_reply":"2021-09-30T18:47:29.944436Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-09-30T18:47:29.948531Z","iopub.execute_input":"2021-09-30T18:47:29.948806Z","iopub.status.idle":"2021-09-30T18:47:30.152844Z","shell.execute_reply.started":"2021-09-30T18:47:29.948758Z","shell.execute_reply":"2021-09-30T18:47:30.152154Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.154932Z","iopub.execute_input":"2021-09-30T18:47:30.155147Z","iopub.status.idle":"2021-09-30T18:47:30.176664Z","shell.execute_reply.started":"2021-09-30T18:47:30.155103Z","shell.execute_reply":"2021-09-30T18:47:30.175431Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"df_train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.178149Z","iopub.execute_input":"2021-09-30T18:47:30.178444Z","iopub.status.idle":"2021-09-30T18:47:30.199774Z","shell.execute_reply.started":"2021-09-30T18:47:30.178356Z","shell.execute_reply":"2021-09-30T18:47:30.198932Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.200812Z","iopub.execute_input":"2021-09-30T18:47:30.201066Z","iopub.status.idle":"2021-09-30T18:47:30.218701Z","shell.execute_reply.started":"2021-09-30T18:47:30.201024Z","shell.execute_reply":"2021-09-30T18:47:30.217897Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"df_test.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.219698Z","iopub.execute_input":"2021-09-30T18:47:30.219945Z","iopub.status.idle":"2021-09-30T18:47:30.235903Z","shell.execute_reply.started":"2021-09-30T18:47:30.219886Z","shell.execute_reply":"2021-09-30T18:47:30.235119Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"sample_submission.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.237397Z","iopub.execute_input":"2021-09-30T18:47:30.237597Z","iopub.status.idle":"2021-09-30T18:47:30.246740Z","shell.execute_reply.started":"2021-09-30T18:47:30.237558Z","shell.execute_reply":"2021-09-30T18:47:30.245437Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"sample_submission.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.248025Z","iopub.execute_input":"2021-09-30T18:47:30.248244Z","iopub.status.idle":"2021-09-30T18:47:30.264410Z","shell.execute_reply.started":"2021-09-30T18:47:30.248199Z","shell.execute_reply":"2021-09-30T18:47:30.262961Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем, где у нас обучение\ndf_test['sample'] = 0 # помечаем, где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, поэтому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.266168Z","iopub.execute_input":"2021-09-30T18:47:30.266491Z","iopub.status.idle":"2021-09-30T18:47:30.312428Z","shell.execute_reply.started":"2021-09-30T18:47:30.266425Z","shell.execute_reply":"2021-09-30T18:47:30.311375Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.314123Z","iopub.execute_input":"2021-09-30T18:47:30.314406Z","iopub.status.idle":"2021-09-30T18:47:30.338735Z","shell.execute_reply.started":"2021-09-30T18:47:30.314360Z","shell.execute_reply":"2021-09-30T18:47:30.337736Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана","metadata":{}},{"cell_type":"code","source":"data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.340060Z","iopub.execute_input":"2021-09-30T18:47:30.340278Z","iopub.status.idle":"2021-09-30T18:47:30.358779Z","shell.execute_reply.started":"2021-09-30T18:47:30.340244Z","shell.execute_reply":"2021-09-30T18:47:30.357879Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"data.Reviews[1]","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.360189Z","iopub.execute_input":"2021-09-30T18:47:30.360530Z","iopub.status.idle":"2021-09-30T18:47:30.375680Z","shell.execute_reply.started":"2021-09-30T18:47:30.360484Z","shell.execute_reply":"2021-09-30T18:47:30.374417Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки.","metadata":{}},{"cell_type":"markdown","source":"# Cleaning and Prepping Data\nОбычно данные содержат в себе кучу мусора, который необходимо почистить, для того чтобы привести их в приемлемый формат. Чистка данных — это необходимый этап решения почти любой реальной задачи.   \n<!-- ![](https://analyticsindiamag.com/wp-content/uploads/2018/01/data-cleaning.png) -->","metadata":{}},{"cell_type":"markdown","source":"## 1. Обработка NAN \nУ наличия пропусков могут быть разные причины, но пропуски нужно либо заполнить, либо исключить из набора полностью. Но с пропусками нужно быть внимательным, **даже отсутствие информации может быть важным признаком!**   \nПо этому перед обработкой NAN лучше вынести информацию о наличии пропуска как отдельный признак ","metadata":{}},{"cell_type":"markdown","source":"Приведем названия столбцов датасета к более лаконичному виду, удалим ненужные столбцы:","metadata":{}},{"cell_type":"code","source":"data.sample()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.377070Z","iopub.execute_input":"2021-09-30T18:47:30.377388Z","iopub.status.idle":"2021-09-30T18:47:30.401121Z","shell.execute_reply.started":"2021-09-30T18:47:30.377322Z","shell.execute_reply":"2021-09-30T18:47:30.400355Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"data.drop(['URL_TA', 'ID_TA'], axis=1, inplace=True)\ndata.columns = ['id', 'city', 'cuisine', 'ranking', 'price_range', 'rev_number', 'reviews', 'sample', 'rating']","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.402251Z","iopub.execute_input":"2021-09-30T18:47:30.402572Z","iopub.status.idle":"2021-09-30T18:47:30.423276Z","shell.execute_reply.started":"2021-09-30T18:47:30.402540Z","shell.execute_reply":"2021-09-30T18:47:30.422671Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# Для примера я возьму столбец Number of Reviews\ndata['Number_of_Reviews_isNAN'] = pd.isna(data['rev_number']).astype('uint8')","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.424283Z","iopub.execute_input":"2021-09-30T18:47:30.424655Z","iopub.status.idle":"2021-09-30T18:47:30.431254Z","shell.execute_reply.started":"2021-09-30T18:47:30.424622Z","shell.execute_reply":"2021-09-30T18:47:30.430064Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"data['Number_of_Reviews_isNAN']","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.432383Z","iopub.execute_input":"2021-09-30T18:47:30.432622Z","iopub.status.idle":"2021-09-30T18:47:30.447401Z","shell.execute_reply.started":"2021-09-30T18:47:30.432571Z","shell.execute_reply":"2021-09-30T18:47:30.446625Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# Заполняем пропуски средним по городу\n\nmean_NoR_by_city = dict([\n      (city, data.loc[data.loc[:, 'city'] == city, 'rev_number'].mean())\\\n      for city in data.loc[:, 'city'].unique()\n])\n\ndata.loc[:, 'rev_number'].\\\n    fillna(data.loc[:, 'city'].\\\n           apply(lambda c: mean_NoR_by_city[c]), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.454160Z","iopub.execute_input":"2021-09-30T18:47:30.454576Z","iopub.status.idle":"2021-09-30T18:47:30.564655Z","shell.execute_reply.started":"2021-09-30T18:47:30.454542Z","shell.execute_reply":"2021-09-30T18:47:30.564160Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"### 2. Обработка признаков\nДля начала посмотрим какие признаки у нас могут быть категориальными.","metadata":{}},{"cell_type":"code","source":"data.nunique(dropna=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.566192Z","iopub.execute_input":"2021-09-30T18:47:30.566638Z","iopub.status.idle":"2021-09-30T18:47:30.627245Z","shell.execute_reply.started":"2021-09-30T18:47:30.566595Z","shell.execute_reply":"2021-09-30T18:47:30.626781Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"Какие признаки можно считать категориальными?","metadata":{}},{"cell_type":"markdown","source":"Для кодирования категориальных признаков есть множество подходов:\n* Label Encoding\n* One-Hot Encoding\n* Target Encoding\n* Hashing\n\nВыбор кодирования зависит от признака и выбраной модели.\nНе будем сейчас сильно погружаться в эту тематику, давайте посмотрим лучше пример с One-Hot Encoding:\n<!-- ![](https://i.imgur.com/mtimFxh.png) -->","metadata":{}},{"cell_type":"code","source":"#One-Hot Encoding для топа городов\ntop_cities_num = 5 #можно установить любое\ntop_cities = data.loc[:, 'city'].value_counts()[0:top_cities_num].index\ntop_cities = list(map(lambda x: '_' + x, top_cities))\ndata = pd.concat([\n         data,\n         pd.get_dummies(data, columns=['city'], prefix='').\n         loc[:, top_cities]],\n         axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.628127Z","iopub.execute_input":"2021-09-30T18:47:30.628413Z","iopub.status.idle":"2021-09-30T18:47:30.669423Z","shell.execute_reply.started":"2021-09-30T18:47:30.628382Z","shell.execute_reply":"2021-09-30T18:47:30.668908Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.670303Z","iopub.execute_input":"2021-09-30T18:47:30.670593Z","iopub.status.idle":"2021-09-30T18:47:30.687184Z","shell.execute_reply.started":"2021-09-30T18:47:30.670563Z","shell.execute_reply":"2021-09-30T18:47:30.686042Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.688340Z","iopub.execute_input":"2021-09-30T18:47:30.688706Z","iopub.status.idle":"2021-09-30T18:47:30.718562Z","shell.execute_reply.started":"2021-09-30T18:47:30.688665Z","shell.execute_reply":"2021-09-30T18:47:30.717968Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"#### Возьмем следующий признак \"Price Range\".","metadata":{}},{"cell_type":"code","source":"data['price_range'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.719669Z","iopub.execute_input":"2021-09-30T18:47:30.720056Z","iopub.status.idle":"2021-09-30T18:47:30.734227Z","shell.execute_reply.started":"2021-09-30T18:47:30.720015Z","shell.execute_reply":"2021-09-30T18:47:30.733311Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"По описанию 'Price Range' это - Цены в ресторане.  \nИх можно поставить по возрастанию (значит это не категориальный признак). А это значит, что их можно заменить последовательными числами, например 1,2,3  \n*Попробуйте сделать обработку этого признака уже самостоятельно!*","metadata":{}},{"cell_type":"code","source":"# моя обработка 'Price Range'\nprice_range_quantize = {\n    '$': 1, '$$ - $$$': 2, '$$$$': 3\n}\ndata.loc[:, 'price_range']=\\\ndata.loc[:, 'price_range'].apply(lambda x: 0 if pd.isna(x) else price_range_quantize[x])","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.735497Z","iopub.execute_input":"2021-09-30T18:47:30.735891Z","iopub.status.idle":"2021-09-30T18:47:30.786295Z","shell.execute_reply.started":"2021-09-30T18:47:30.735791Z","shell.execute_reply":"2021-09-30T18:47:30.785756Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"> Для некоторых алгоритмов МЛ даже для не категориальных признаков можно применить One-Hot Encoding, и это может улучшить качество модели. Пробуйте разные подходы к кодированию признака - никто не знает заранее, что может взлететь.","metadata":{}},{"cell_type":"markdown","source":"### Обработать другие признаки вы должны самостоятельно!\nДля обработки других признаков вам возможно придется даже написать свою функцию, а может даже и не одну, но в этом и есть ваша практика в этом модуле!     \nСледуя подсказкам в модуле вы сможете более подробно узнать, как сделать эти приобразования.","metadata":{}},{"cell_type":"code","source":"# тут ваш код на обработку других признаков\n# обработка Cuisine Style\n# превращаем значения Cuisine Style из строк в списки со строковыми элементами\ndata.loc[:, 'cuisine'] =\\\ndata.loc[:, 'cuisine'].fillna(value='[]').apply(\n    lambda cuisine_list_for_id: \\\n    [s.replace(\"'\", \"\") for s in cuisine_list_for_id[2:-2].split(', ')]\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:30.787314Z","iopub.execute_input":"2021-09-30T18:47:30.787661Z","iopub.status.idle":"2021-09-30T18:47:30.999431Z","shell.execute_reply.started":"2021-09-30T18:47:30.787617Z","shell.execute_reply":"2021-09-30T18:47:30.998351Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# обработка Reviews\n# функция для извлечения дат двух последних отзывов\ndef rev_dates(s):\n    date_regex = re.compile(r'(\\d+/\\d+/\\d+)') # компилируем регулярное выражение для поиска дат в формате dd/mm/YYYY\n    str_date_ls = date_regex.findall(s)\n    if str_date_ls != []:\n        enum = list(enumerate(str_date_ls))\n        f = ['%m/%d/%Y' if int(e[1][0:2].replace('/', '')) <= 12 else '%d/%m/%Y' for e in enum]\n        return [dat.strptime(e[1], f[e[0]]) for e in enum]\n    else:\n        return [dat(1970, 1, 1, 0, 0)] # в случае если не нашли дату\n\n# принимает список дат;\n# возвращает разность в днях между двумя последними отзывами\n# если дата только одна, возвращается 0\ndef last_rev_date_delta(dates):\n    if len(dates) >= 3:\n        return abs(dates[2] - dates[1]).days\n    else:\n        return abs(dates[0] - dates[-1]).days\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:31.000883Z","iopub.execute_input":"2021-09-30T18:47:31.001092Z","iopub.status.idle":"2021-09-30T18:47:31.009557Z","shell.execute_reply.started":"2021-09-30T18:47:31.001051Z","shell.execute_reply":"2021-09-30T18:47:31.008397Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"# EDA \n[Exploratory Data Analysis](https://ru.wikipedia.org/wiki/Разведочный_анализ_данных) - Анализ данных\nНа этом этапе мы строим графики, ищем закономерности, аномалии, выбросы или связи между признаками.\nВ общем цель этого этапа понять, что эти данные могут нам дать и как признаки могут быть взаимосвязаны между собой.\nПонимание изначальных признаков позволит сгенерировать новые, более сильные и, тем самым, сделать нашу модель лучше.\n<!-- ![](https://miro.medium.com/max/2598/1*RXdMb7Uk6mGqWqPguHULaQ.png) -->","metadata":{}},{"cell_type":"markdown","source":"### Посмотрим распределение признака Ranking","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,7)\ndf_train['Ranking'].hist(bins=100);","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:31.011453Z","iopub.execute_input":"2021-09-30T18:47:31.011801Z","iopub.status.idle":"2021-09-30T18:47:31.477808Z","shell.execute_reply.started":"2021-09-30T18:47:31.011733Z","shell.execute_reply":"2021-09-30T18:47:31.476863Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?","metadata":{}},{"cell_type":"code","source":"df_train['City'].value_counts(ascending=True).plot(kind='barh');","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:31.479296Z","iopub.execute_input":"2021-09-30T18:47:31.479578Z","iopub.status.idle":"2021-09-30T18:47:31.865407Z","shell.execute_reply.started":"2021-09-30T18:47:31.479536Z","shell.execute_reply":"2021-09-30T18:47:31.864598Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"df_train['Ranking'][df_train['City'] =='London'].hist(bins=100);","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:31.866512Z","iopub.execute_input":"2021-09-30T18:47:31.866857Z","iopub.status.idle":"2021-09-30T18:47:32.242578Z","shell.execute_reply.started":"2021-09-30T18:47:31.866802Z","shell.execute_reply":"2021-09-30T18:47:32.241950Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# посмотрим на топ 10 городов\nfor x in (df_train['City'].value_counts())[0:10].index:\n    df_train['Ranking'][df_train['City'] == x].hist(bins=80)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:32.243617Z","iopub.execute_input":"2021-09-30T18:47:32.243957Z","iopub.status.idle":"2021-09-30T18:47:34.037318Z","shell.execute_reply.started":"2021-09-30T18:47:32.243919Z","shell.execute_reply":"2021-09-30T18:47:34.036634Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n","metadata":{}},{"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной","metadata":{}},{"cell_type":"code","source":"df_train['Rating'].value_counts(ascending=True).plot(kind='barh');","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:34.038363Z","iopub.execute_input":"2021-09-30T18:47:34.038696Z","iopub.status.idle":"2021-09-30T18:47:34.243377Z","shell.execute_reply.started":"2021-09-30T18:47:34.038658Z","shell.execute_reply":"2021-09-30T18:47:34.242741Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"### ***Целевая переменная принимает значения, только кратные 0.5, что можно будет использовать для улучшения метрики предсказания***","metadata":{}},{"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной относительно признака","metadata":{}},{"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100);","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:34.244406Z","iopub.execute_input":"2021-09-30T18:47:34.244740Z","iopub.status.idle":"2021-09-30T18:47:34.623773Z","shell.execute_reply.started":"2021-09-30T18:47:34.244703Z","shell.execute_reply":"2021-09-30T18:47:34.623201Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100);","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:34.624784Z","iopub.execute_input":"2021-09-30T18:47:34.625126Z","iopub.status.idle":"2021-09-30T18:47:35.005277Z","shell.execute_reply.started":"2021-09-30T18:47:34.625088Z","shell.execute_reply":"2021-09-30T18:47:35.004703Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"df_train['Ranking'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:35.006271Z","iopub.execute_input":"2021-09-30T18:47:35.006595Z","iopub.status.idle":"2021-09-30T18:47:35.017539Z","shell.execute_reply.started":"2021-09-30T18:47:35.006558Z","shell.execute_reply":"2021-09-30T18:47:35.016860Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"### Тепловая карта корреляции признаков","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.drop(['sample'], axis=1).corr(), annot = True, cmap = 'Spectral_r');","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:35.018689Z","iopub.execute_input":"2021-09-30T18:47:35.019080Z","iopub.status.idle":"2021-09-30T18:47:35.668849Z","shell.execute_reply.started":"2021-09-30T18:47:35.019041Z","shell.execute_reply":"2021-09-30T18:47:35.667878Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию.","metadata":{}},{"cell_type":"code","source":"# на всякий случай, заново подгружаем данные\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:35.670004Z","iopub.execute_input":"2021-09-30T18:47:35.670330Z","iopub.status.idle":"2021-09-30T18:47:35.923147Z","shell.execute_reply.started":"2021-09-30T18:47:35.670292Z","shell.execute_reply":"2021-09-30T18:47:35.922505Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"data.Reviews.apply(type).value_counts()\ndata.Reviews[data.Reviews.apply(type) == float]","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:35.924162Z","iopub.execute_input":"2021-09-30T18:47:35.924373Z","iopub.status.idle":"2021-09-30T18:47:35.949275Z","shell.execute_reply.started":"2021-09-30T18:47:35.924328Z","shell.execute_reply":"2021-09-30T18:47:35.948388Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# класс для предобработки данных датасета для модели\nclass PreprocData:\n    '''includes several functions to pre-process the predictor data.'''\n    \n    def __init__(self, df_input):\n        self.data = df_input.copy()\n\n    def preprocess(self):\n        # ################### 1. Предобработка ############################################################## \n        # убираем не нужные для модели признаки\n        self.data.drop(['URL_TA', 'ID_TA'], axis=1, inplace=True)\n        self.data.columns = ['id', 'city', 'cuisine', 'ranking', 'price_range', 'rev_number', 'reviews', 'sample', 'rating']\n\n        # обработка Cuisine Style\n        # превращаем значения Cuisine Style из строк в списки со строковыми элементами\n        self.data.loc[:, 'cuisine'] =\\\n        self.data.loc[:, 'cuisine'].fillna(value='[]').apply(\n            lambda cuisine_list_for_id: \\\n            [s.replace(\"'\", \"\") for s in cuisine_list_for_id[2:-2].split(', ')]\n        )\n    \n        # обработка Reviews\n        # функция для извлечения дат двух последних отзывов\n        # принимает строку, возвращает список объектов datetime\n        # если в строке не удалось найти дату, возвращает список с единственным\n        # datetime 01.01.1970 00:00\n        def rev_dates(s):\n            date_regex = re.compile(r'(\\d+/\\d+/\\d+)') # компилируем регулярное выражение для поиска дат в формате dd/mm/YYYY\n            str_date_ls = date_regex.findall(s)\n            if str_date_ls != []:\n                enum = list(enumerate(str_date_ls))\n                f = ['%m/%d/%Y' if int(e[1][0:2].replace('/', '')) <= 12 \\\n                     else '%d/%m/%Y' if '/' not in e[1][-4:-1] else '%d/%m/%y' for e in enum]\n                return [dat.strptime(e[1], f[e[0]]) for e in enum]\n            else:\n                return [dat(1970, 1, 1, 0, 0)] # в случае если не нашли дату\n            \n        # добавляем в данные столбец с кол-вом дней между двумя последними отзывами\n        self.data.loc[:, 'reviews'].fillna('', inplace=True)\n        self.data.loc[:, 'rev_dates'] =\\\n            self.data.loc[:, 'reviews'].apply(lambda s: rev_dates(s))\n        \n    \n    def fill_missing(self):\n        \n    # ################### 2. NAN ############################################################## \n        # Заполняем пропуски в Number of Reviews средним по городу\n\n        mean_NoR_by_city = dict([\n          (city, self.data.loc[self.data.loc[:, 'city'] == city, 'rev_number'].mean())\\\n          for city in self.data.loc[:, 'city'].unique()\n        ])\n\n        self.data.loc[:, 'rev_number'].\\\n            fillna(self.data.loc[:, 'city'].\\\n               apply(lambda c: mean_NoR_by_city[c]), inplace=True)\n    #==============================================================================================\n    #заполняем пропуски в price_range - сохраняя ценовое распределение ресторанов в каждом городе\n\n    #     def fill_price(city, city_pr_distr): # distr -> distr[city][distr[city].index != 0]\n    #         borders = [city_pr_distr[0:i+1].sum() for i in range(len(city_pr_distr))]\n        \n    #         pick_rand = randint(0, borders[-1])\n    #         for border in enumerate(borders):\n    #             if pick_rand < border[1]:\n    #                 return city_pr_distr.index[border[0]]\n\n    #     distr = self.data.groupby('city')['price_range'].value_counts()\n\n    #     self.data.loc[dself.data.loc[:, 'price_range'] == 0, 'price_range'] = \\\n    #     self.data.loc[self.data.loc[:, 'price_range'] == 0, 'city'].apply(\n    #         lambda city: fill_price(city, distr[city][distr[city].index != 0])\n    #     )\n\n\n    #     self.data.loc[self.data.loc[:, 'price_range'].apply(pd.isna), 'price_range'] = \\\n    #     self.data.loc[self.data.loc[:, 'price_range'].apply(pd.isna), 'city'].apply(\n    #         lambda city: fill_price(city, distr[city][distr[city].index != 0])\n    #     )\n    \n    #     подобное заполнение Price Range себя не оправдало - заполнение нулями дает лучший результат\n\n    def encode(self):\n    # ################### 3. Encoding ############################################################## \n        #One-Hot Encoding для топа городов\n        \n        top_cities_num = 5 #можно установить любое\n        top_cities = self.data.loc[:, 'city'].value_counts()[0:top_cities_num].index\n        top_cities = list(map(lambda x: '_' + x, top_cities))\n        self.data = pd.concat([\n             self.data,\n             pd.get_dummies(self.data, columns=['city'], prefix='').\n             loc[:, top_cities]],\n             axis=1\n        )\n    #==============================================================================================\n        # One-Hot Encoding для Сuisine Style и прикрепляем его к осн. датафрейму\n        \n        top_cuisines_num = 5  #кол-во самых распространненных кухонь, можно взять любое\n\n        # создаем список самых популярных top_cuisines_num типов кухонь\n        top_cuisines = self.data.loc[self.data.loc[:, 'cuisine'].apply(lambda x: x != ['']), :].\\\n            explode('cuisine').groupby('cuisine')['cuisine'].\\\n                count().sort_values(ascending=False).head(top_cuisines_num).index\n\n        # прикрепляем к датафрейму\n        top_cuisines = list(map(lambda x: '_' + x, top_cuisines))\n        self.data = pd.concat([\n             self.data,\n             pd.get_dummies(self.data.explode('cuisine'), columns=['cuisine'], prefix='').\n             sum(level=0).loc[:, top_cuisines]],\n             axis=1\n        )\n        #==============================================================================================    \n        # Label Encoding для Price Range\n    #     price_range_encode = {\n    #         '$': 1, '$$ - $$$': 2, '$$$$': 3\n    #     }\n    #     self.data.loc[:, 'price_range']=\\\n    #         self.data.loc[:, 'price_range'].apply(lambda x: 0 if pd.isna(x) else price_range_encode)\n    #==============================================================================================    \n        # One-Hot Encoding для Price Range\n\n        price_range_rename = {\n            '$': 'S', '$$ - $$$': 'SSS', '$$$$': 'SSSS' \n        }\n        self.data.loc[:, 'price_range'].fillna('NaN', inplace=True)\n        self.data.loc[:, 'price_range']=\\\n        self.data.loc[:, 'price_range'].apply(lambda x: price_range_rename[x] if x !='NaN' else 'NaN')\n    \n        self.data = pd.get_dummies(self.data, columns=['price_range'], prefix='pr', dummy_na=True) \n    \n    def add_features(self):\n    # ################### 4. Feature Engineering ####################################################\n    # код на генерацию новых признаков\n    # добавляем признак - численность населения города, в котором находится ресторан\n    # данные - из Википедии (население не включающее городскую агломерацию)\n        \n        # добавляем в данные признак - кол-во дней между двумя последними отзывами\n        \n        # принимает список дат;\n        # возвращает разность в днях между двумя последними отзывами\n        # если дата только одна, возвращается 0\n        def last_rev_date_delta(dates):\n            if len(dates) >= 3:\n                return abs(dates[2] - dates[1]).days\n            else:\n                return abs(dates[0] - dates[-1]).days\n            \n        self.data.loc[:, 'delta'] =\\\n            self.data.loc[:, 'rev_dates'].apply(lambda s: last_rev_date_delta(s))\n    \n        population = {\n           'Paris': 2176, 'Stockholm': 1656, 'London': 10840, 'Berlin': 3664,\n           'Munich': 1553, 'Oporto': 232, 'Milan': 3144, 'Bratislava': 433,\n           'Vienna': 1911, 'Rome': 2860, 'Barcelona': 1620, 'Madrid': 3223,\n           'Dublin': 555, 'Brussels': 1209, 'Zurich': 415, 'Warsaw': 1794,\n           'Budapest': 1752, 'Copenhagen': 799, 'Amsterdam': 873, 'Lyon': 516,\n           'Hamburg': 1899, 'Lisbon': 507, 'Prague': 1324, 'Oslo': 697, \n           'Helsinki': 658, 'Edinburgh': 488, 'Geneva': 202, 'Ljubljana': 296,\n           'Athens': 664, 'Luxembourg': 125, 'Krakow': 781}\n        self.data.loc[:, 'population'] = self.data.loc[:, 'city'].apply(lambda city: population[city])\n    #==============================================================================================  \n        # добавляем признак - является ли город столицей: 1 - да, 0 - нет\n    \n        is_capital = {'Paris': 1, 'Stockholm': 1, 'London': 1, 'Berlin': 1,\n           'Munich': 0, 'Oporto': 0, 'Milan': 0, 'Bratislava': 1,\n           'Vienna': 1, 'Rome': 1, 'Barcelona': 0, 'Madrid': 1,\n           'Dublin': 1, 'Brussels': 1, 'Zurich': 0, 'Warsaw': 1,\n           'Budapest': 1, 'Copenhagen': 1, 'Amsterdam': 1, 'Lyon': 0,\n           'Hamburg': 0, 'Lisbon': 1, 'Prague': 1, 'Oslo': 1, \n           'Helsinki': 1, 'Edinburgh': 0, 'Geneva': 0, 'Ljubljana': 1,\n           'Athens': 1, 'Luxembourg': 1, 'Krakow': 0\n        }\n        self.data.loc[:, 'is_capital'] = self.data.loc[:, 'city'].apply(lambda city: is_capital[city])\n    #==============================================================================================    \n        # добавляем признак - с подушевым ВВП по ППС города, в кот. нах. ресторан\n        # данные взяты из https://ec.europa.eu/eurostat/\n    \n        gdp = {\n            'Paris': 61883, 'Stockholm': 61754, 'London': 58827, 'Berlin': 37601,\n            'Munich': 69844, 'Oporto': 24819, 'Milan': 51768, 'Bratislava': 67841,\n            'Vienna': 46787, 'Rome': 41475, 'Barcelona': 45752, 'Madrid': 43074,\n            'Dublin':30000, 'Brussels': 54634, 'Zurich': 64302, 'Warsaw': 49722,\n            'Budapest': 37399, 'Copenhagen': 54197, 'Amsterdam': 60857, 'Lyon': 46913,\n            'Hamburg': 52947, 'Lisbon': 34782, 'Prague': 48160, 'Oslo': 64673,\n            'Helsinki': 49760, 'Edinburgh': 44059, 'Geneva': 62115, 'Ljubljana': 39763,\n            'Athens': 32167, 'Luxembourg': 88312, 'Krakow': 29695\n        }\n        self.data.loc[:, 'gdp'] = self.data.loc[:, 'city'].apply(lambda city: gdp[city])\n    #==============================================================================================\n        #добавляем признак - количество предприятий общепита в стране нахождения ресторана\n        # данные взяты из https://ec.europa.eu/eurostat/ \n        n_of_food_serv = {\n            'Paris': 161466, 'Stockholm': 23208, 'London': 88841, 'Berlin': 136091,\n            'Munich': 136091, 'Oporto': 31363, 'Milan': 155875, 'Bratislava': 10917,\n            'Vienna': 27348, 'Rome': 155875, 'Barcelona': 72657, 'Madrid': 72657,\n            'Dublin': 7993, 'Brussels': 30773, 'Zurich': 11961, 'Warsaw': 39291,\n            'Budapest': 16850, 'Copenhagen': 8983, 'Amsterdam': 30502, 'Lyon': 161466,\n            'Hamburg': 136091, 'Lisbon': 31363, 'Prague': 45623, 'Oslo': 6605,\n            'Helsinki': 7769, 'Edinburgh': 88841, 'Geneva': 11961, 'Ljubljana': 4369,\n            'Athens': 39978, 'Luxembourg': 1475, 'Krakow': 39291\n        }\n        self.data.loc[:, 'n_of_food_serv'] = self.data.loc[:, 'city'].\\\n            apply(lambda city: n_of_food_serv[city])\n    #==============================================================================================\n        #количество ресторанов в городе согласно основному датасету на душу населения города\n        n_of_rest = self.data.loc[:, 'city'].value_counts()\n\n        self.data.loc[:, 'n_of_rest'] = self.data.loc[:, 'city'].\\\n            apply(lambda city: n_of_rest[city])\n\n        self.data.loc[:, 'n_of_rest_per_capita'] = self.data.loc[:, 'n_of_rest'] /\\\n            self.data.loc[:, 'population']\n    #==============================================================================================\n        # добавляем признак суммарного ranking по городам\n    \n        ranking_sum_by_city = self.data.groupby('city').sum()['ranking']\n        self.data.loc[:, 'ranking_summated'] = self.data.loc[:, 'city'].\\\n            apply(lambda city: ranking_sum_by_city[city])    \n    #==============================================================================================\n        pf = PolynomialFeatures(3)\n        poly_features = pf.fit_transform(\n            self.data.loc[:, ['rev_number', 'ranking']]\n        )\n    \n        self.data = pd.concat([\n             self.data,\n             pd.DataFrame(poly_features)\n             ],\n             axis=1\n        )\n    \n    \n    def clean(self):\n    # ################### 5. Clean #################################################### \n        # убираем признаки которые еще не успели обработать, \n        # модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n        object_columns = [s for s in self.data.columns if self.data[s].dtypes == 'object']\n        self.data.drop(object_columns, axis = 1, inplace=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:35.950412Z","iopub.execute_input":"2021-09-30T18:47:35.950625Z","iopub.status.idle":"2021-09-30T18:47:35.995529Z","shell.execute_reply.started":"2021-09-30T18:47:35.950579Z","shell.execute_reply":"2021-09-30T18:47:35.994704Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"#### Запускаем и проверяем, что получилось","metadata":{}},{"cell_type":"code","source":"df_preproc = PreprocData(data)\n\ndf_preproc.preprocess()\n\ndf_preproc.fill_missing()\n\ndf_preproc.encode()\n\ndf_preproc.add_features()\n\ndf_preproc.clean()\n\ndf_preproc = df_preproc.data\ndf_preproc.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:35.996493Z","iopub.execute_input":"2021-09-30T18:47:35.996816Z","iopub.status.idle":"2021-09-30T18:47:39.440567Z","shell.execute_reply.started":"2021-09-30T18:47:35.996771Z","shell.execute_reply":"2021-09-30T18:47:39.439395Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"df_preproc.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:39.441799Z","iopub.execute_input":"2021-09-30T18:47:39.442039Z","iopub.status.idle":"2021-09-30T18:47:39.456697Z","shell.execute_reply.started":"2021-09-30T18:47:39.441993Z","shell.execute_reply":"2021-09-30T18:47:39.455659Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"### Посмотрим визуализацию корреляций в получившемся датафрейме:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (25, 25))\n\nsns.heatmap(df_preproc.corr(), annot = True, cmap = 'Spectral_r');","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:39.457756Z","iopub.execute_input":"2021-09-30T18:47:39.457991Z","iopub.status.idle":"2021-09-30T18:47:44.001437Z","shell.execute_reply.started":"2021-09-30T18:47:39.457949Z","shell.execute_reply":"2021-09-30T18:47:44.000801Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"### Удалим сильно кореллирующие признаки и еще раз визуализируем корелляции в данных:","metadata":{}},{"cell_type":"code","source":"df_model = df_preproc.drop([\n    'population', 'n_of_rest', '_London', 'pr_nan', 'sample',\n     0, 1, 2, 3, 5, 8, 9 \n],\n     axis=1\n)\n# 3, 5, 6, 8, 9\nplt.figure(figsize = (15, 15))\nsns.heatmap(df_model.corr(), annot = True, cmap = 'Spectral_r');\ndf_model['sample'] = df_preproc['sample']","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:44.002572Z","iopub.execute_input":"2021-09-30T18:47:44.002958Z","iopub.status.idle":"2021-09-30T18:47:46.711342Z","shell.execute_reply.started":"2021-09-30T18:47:44.002921Z","shell.execute_reply":"2021-09-30T18:47:46.710561Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"df_model.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:46.712685Z","iopub.execute_input":"2021-09-30T18:47:46.713144Z","iopub.status.idle":"2021-09-30T18:47:46.747058Z","shell.execute_reply.started":"2021-09-30T18:47:46.713098Z","shell.execute_reply":"2021-09-30T18:47:46.746101Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_model.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_model.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.rating.values            # наш таргет\nX = train_data.drop(['rating'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:46.748356Z","iopub.execute_input":"2021-09-30T18:47:46.748695Z","iopub.status.idle":"2021-09-30T18:47:46.771485Z","shell.execute_reply.started":"2021-09-30T18:47:46.748659Z","shell.execute_reply":"2021-09-30T18:47:46.770847Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**","metadata":{}},{"cell_type":"code","source":"# Воспользуемся специальной функцией train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:46.772493Z","iopub.execute_input":"2021-09-30T18:47:46.772807Z","iopub.status.idle":"2021-09-30T18:47:46.785205Z","shell.execute_reply.started":"2021-09-30T18:47:46.772773Z","shell.execute_reply":"2021-09-30T18:47:46.784310Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:46.786368Z","iopub.execute_input":"2021-09-30T18:47:46.786594Z","iopub.status.idle":"2021-09-30T18:47:46.797694Z","shell.execute_reply.started":"2021-09-30T18:47:46.786549Z","shell.execute_reply":"2021-09-30T18:47:46.796679Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"# Model \nСам ML","metadata":{}},{"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:46.799067Z","iopub.execute_input":"2021-09-30T18:47:46.799426Z","iopub.status.idle":"2021-09-30T18:47:46.812705Z","shell.execute_reply.started":"2021-09-30T18:47:46.799252Z","shell.execute_reply":"2021-09-30T18:47:46.811782Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:46.813894Z","iopub.execute_input":"2021-09-30T18:47:46.814252Z","iopub.status.idle":"2021-09-30T18:47:46.827789Z","shell.execute_reply.started":"2021-09-30T18:47:46.814172Z","shell.execute_reply":"2021-09-30T18:47:46.826749Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:46.831121Z","iopub.execute_input":"2021-09-30T18:47:46.831354Z","iopub.status.idle":"2021-09-30T18:47:53.707297Z","shell.execute_reply.started":"2021-09-30T18:47:46.831305Z","shell.execute_reply":"2021-09-30T18:47:53.706027Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"y_pred = (y_pred*2).round()/2","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:53.708574Z","iopub.execute_input":"2021-09-30T18:47:53.708824Z","iopub.status.idle":"2021-09-30T18:47:53.713443Z","shell.execute_reply.started":"2021-09-30T18:47:53.708781Z","shell.execute_reply":"2021-09-30T18:47:53.712783Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:53.714290Z","iopub.execute_input":"2021-09-30T18:47:53.714575Z","iopub.status.idle":"2021-09-30T18:47:53.728585Z","shell.execute_reply.started":"2021-09-30T18:47:53.714545Z","shell.execute_reply":"2021-09-30T18:47:53.728149Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh', log=True);","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:53.729442Z","iopub.execute_input":"2021-09-30T18:47:53.729721Z","iopub.status.idle":"2021-09-30T18:47:54.387273Z","shell.execute_reply.started":"2021-09-30T18:47:53.729691Z","shell.execute_reply":"2021-09-30T18:47:54.386002Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на kaggle","metadata":{}},{"cell_type":"code","source":"test_data.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:54.388422Z","iopub.execute_input":"2021-09-30T18:47:54.388636Z","iopub.status.idle":"2021-09-30T18:47:54.423042Z","shell.execute_reply.started":"2021-09-30T18:47:54.388599Z","shell.execute_reply":"2021-09-30T18:47:54.422330Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"test_data = test_data.drop(['rating'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:54.424163Z","iopub.execute_input":"2021-09-30T18:47:54.424367Z","iopub.status.idle":"2021-09-30T18:47:54.431715Z","shell.execute_reply.started":"2021-09-30T18:47:54.424334Z","shell.execute_reply":"2021-09-30T18:47:54.430409Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:54.433199Z","iopub.execute_input":"2021-09-30T18:47:54.433451Z","iopub.status.idle":"2021-09-30T18:47:54.454532Z","shell.execute_reply.started":"2021-09-30T18:47:54.433404Z","shell.execute_reply":"2021-09-30T18:47:54.451666Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"predict_submission = model.predict(test_data)\npredict_submission = (predict_submission*2).round()/2","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:54.456156Z","iopub.execute_input":"2021-09-30T18:47:54.456413Z","iopub.status.idle":"2021-09-30T18:47:54.568249Z","shell.execute_reply.started":"2021-09-30T18:47:54.456364Z","shell.execute_reply":"2021-09-30T18:47:54.567076Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"predict_submission","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:54.569779Z","iopub.execute_input":"2021-09-30T18:47:54.570110Z","iopub.status.idle":"2021-09-30T18:47:54.576058Z","shell.execute_reply.started":"2021-09-30T18:47:54.570050Z","shell.execute_reply":"2021-09-30T18:47:54.575479Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:47:54.576951Z","iopub.execute_input":"2021-09-30T18:47:54.577260Z","iopub.status.idle":"2021-09-30T18:47:54.614434Z","shell.execute_reply.started":"2021-09-30T18:47:54.577218Z","shell.execute_reply":"2021-09-30T18:47:54.613928Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"# What's next?\nИли что делать, чтоб улучшить результат:\n* Обработать оставшиеся признаки в понятный для машины формат\n* Посмотреть, что еще можно извлечь из признаков\n* Сгенерировать новые признаки\n* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n* Подобрать состав признаков\n\nВ общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n","metadata":{}}]}
